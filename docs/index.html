<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Sports vs Politics — Text Classifier</title>
  <meta name="description" content="A classical NLP baseline study: BoW/TF-IDF + 3 ML models to classify documents as Sports or Politics." />
  <link rel="stylesheet" href="styles.css" />
</head>

<body>
  <header class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <span class="dot"></span>
        <span>Sports vs Politics Classifier</span>
      </div>
      <nav class="topnav">
        <a href="#dataset">Dataset</a>
        <a href="#eda">EDA</a>
        <a href="#methods">Methods</a>
        <a href="#setup">Experimental Setup</a>
        <a href="#conclusion">Conclusions</a>
        <a href="#refs">References</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <article class="post">
      <section class="hero">
        <h1>Sports vs Politics — Document Classification</h1>
        <p class="subtitle">
          A classical NLP baseline study using sparse text features (BoW / TF-IDF / n-grams) and 3+ ML models.
        </p>

        <div class="meta">
          <span>Author: <strong>Omprakash Nain</strong></span>
          <span>Course: <strong>Natural Language Understanding</strong></span>
          <span>Date: <strong>Feb 15 2026</strong></span>
        </div>

        <div class="callout">
          <strong>Goal:</strong> Build a classifier that reads a text document and labels it as either
          <strong>Sports</strong> or <strong>Politics</strong>, then compare at least three ML techniques quantitatively.
        </div>
      </section>

      <aside class="toc">
        <h2>Table of contents</h2>
        <ol>
            <li><a href="#problem">Problem</a></li>
            <li><a href="#dataset">Dataset</a></li>
            <li><a href="#eda">EDA</a></li>
            <li><a href="#methods">Methods</a></li>
            <li><a href="#setup">Experimental Setup</a></li>
            <li><a href="#conclusion">Conclusions</a></li>
            <li><a href="#refs">References</a></li>
        </ol>
      </aside>

      <section id="problem">
        <h2>1. Problem</h2>
        <p>
          We solve a binary text classification task:
          given a document <em>x</em>, predict <em>y ∈ {sports, politics}</em>.
        </p>
        <p>
          This project intentionally uses simple, explainable NLP baselines (BoW/TF-IDF + linear models),
          because they are strong for topic classification and easy to analyze.
        </p>
      </section>

      <section id="dataset">
        <h2>2. Dataset</h2>

        <p>
            We use the BBC News Topic dataset (2004–2005), a popular benchmark for topic classification.
            It contains <strong>2,225</strong> articles labeled into five categories:
            business, entertainment, politics, sport, and tech.
        </p>

        <div class="callout">
            <strong>Binary task construction:</strong> We filter the dataset to only
            <code>sport</code> and <code>politics</code> to match the assignment statement exactly.
            The resulting dataset contains <strong>928 documents</strong>:
            <strong>511 sport</strong> and <strong>417 politics</strong>.
        </div>

        <div class="grid2">
            <div class="card">
            <h3>Data source</h3>
            <ul>
                <li>
                Practical download: Kaggle CSV commonly distributed as <code>bbc-text.csv</code>
                with columns like <code>category</code> and <code>text</code>.
                </li>
                <li>
                Original dataset distribution is maintained by Derek Greene as “BBC Datasets”
                for research/benchmarking.
                </li>
                <li>
                Copyright note: the original article content is owned by the BBC, so we do not
                re-upload the raw text in this repository.
                </li>
            </ul>
            </div>

            <div class="card">
            <h3>Schema</h3>
            <table>
                <thead>
                <tr>
                    <th>Column</th>
                    <th>Meaning</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td><code>category</code></td>
                    <td>One of {business, entertainment, politics, sport, tech}</td>
                </tr>
                <tr>
                    <td><code>text</code></td>
                    <td>Full article text</td>
                </tr>
                </tbody>
            </table>
            <p class="footnote">
                We only keep rows where <code>category ∈ {sport, politics}</code>.
            </p>
            </div>
        </div>

        <div class="card">
            <h3>Collection and curation procedure</h3>
            <ol>
            <li><strong>Download</strong> the CSV dataset locally (do not commit it to GitHub).</li>
            <li><strong>Validate schema</strong>: ensure required columns exist and remove null/empty texts.</li>
            <li><strong>Deduplicate</strong> (recommended): drop exact duplicates of <code>text</code>.</li>
            <li><strong>Filter classes</strong>: keep only <code>sport</code> and <code>politics</code>.</li>
            <li>
                <strong>Split</strong>: use a stratified split to preserve class ratio
                (e.g., 80/20 with a fixed random seed) or stratified k-fold cross-validation.
            </li>
            </ol>
        </div>

        <figure class="figure">
            <img src="assets/figures/class_balance.png" alt="Class balance bar chart" />
            <figcaption>
            <strong>Figure:</strong> Class distribution after filtering (sport vs politics).
            </figcaption>
        </figure>

        <figure class="figure">
            <img src="assets/figures/word_count_dist.png" alt="Word count distribution plot" />
            <figcaption>
            <strong>Figure:</strong> Document length distribution (word counts) by class.
            </figcaption>
        </figure>
        </section>

      <section id="eda">
        <h2>3. Exploratory Data Analysis (EDA)</h2>

        <p>
            Before training models, we inspect dataset balance, document-length patterns, and vocabulary signals.
            The goal is to sanity-check the labels and choose sensible text representations (BoW vs TF-IDF, unigrams vs n-grams).
        </p>

        <div class="grid2">
            <div class="card" style="background:#e6f4ff;border:1px solid #b6dbff;color:#111318;">
            <h3 style="color:#111318;">What we check in EDA</h3>
            <ul style="color:#111318;">
                <li><strong>Balance:</strong> enough samples per class? (See the Dataset section.)</li>
                <li><strong>Length:</strong> do classes differ in typical size? (See the Dataset section.)</li>
                <li><strong>Vocabulary:</strong> do frequent words match the intended topic?</li>
                <li><strong>Ambiguity:</strong> are there mixed-topic documents (sports governance, elections in sports bodies, etc.)?</li>
            </ul>
            </div>

            <div class="card" style="background:#e6f4ff;border:1px solid #b6dbff;color:#111318;">
            <h3 style="color:#111318;">EDA-driven design decisions</h3>
            <ul style="color:#111318;">
                <li>Use <strong>stratified splits</strong> to preserve the class ratio.</li>
                <li>Report <strong>Macro-F1</strong> alongside Accuracy (more robust if imbalance exists).</li>
                <li>Prefer <strong>TF-IDF</strong> as a strong baseline for sparse text.</li>
                <li>Compare <strong>unigrams</strong> vs <strong>1–2 grams</strong> (phrases can be sharp signals).</li>
                <li>Keep preprocessing minimal by default; treat stemming/lemmatization as an ablation.</li>
            </ul>
            </div>
        </div>

        <div class="callout" style="background:#e6f4ff;border:1px solid #b6dbff;color:#111318;">
            <strong>Note:</strong> We already include the class balance and document-length plots in the
            <a href="#dataset">Dataset section</a>. Here, we focus on qualitative vocabulary sanity checks.
        </div>

        <h3>EDA: Vocabulary snapshot via word clouds</h3>
        <p>
            Word clouds provide a fast qualitative view of frequent vocabulary within each class. They are not a
            substitute for quantitative feature importance, but they help confirm that the dataset labels “look right”.
        </p>

        <div class="grid2">
            <figure class="figure">
            <img src="assets/figures/wordcloud_sport.png" alt="Word cloud for Sport class" />
            <figcaption><strong>EDA:</strong> Word cloud for <code>sport</code>.</figcaption>
            </figure>

            <figure class="figure">
            <img src="assets/figures/wordcloud_politics.png" alt="Word cloud for Politics class" />
            <figcaption><strong>EDA:</strong> Word cloud for <code>politics</code>.</figcaption>
            </figure>
        </div>

        <div class="callout" style="background:#e6f4ff;border:1px solid #b6dbff;color:#111318;">
            <strong>Interpretation guide:</strong>
            If the politics cloud is dominated by institutional/election terms and the sport cloud is dominated by match/team/event terms,
            the dataset is consistent with the task. Overlap tokens (e.g., “win”, country names, “campaign”) hint at likely confusion cases.
        </div>

        <div class="card" style="background:#e6f4ff;border:1px solid #b6dbff;color:#111318;">
            <h3 style="color:#111318;">EDA summary</h3>
            <ul style="color:#111318;">
            <li>We avoid relying on document length as a shortcut signal.</li>
            <li>Vocabulary/phrase patterns motivate BoW/TF-IDF + (1–2)-gram experiments.</li>
            <li>Next, we compare at least three ML models under a consistent evaluation protocol.</li>
            </ul>
        </div>
        </section>

      <section id="methods">
        <h2>4. Methods</h2>

        <p>
            This project uses classical supervised machine learning for text classification.
            We convert raw documents into sparse feature vectors (BoW / TF-IDF / n-grams),
            then train multiple classifiers and compare them quantitatively under the same evaluation protocol.
        </p>

        <div class="grid2">
            <div class="card">
            <h3>Text preprocessing</h3>
            <ul>
                <li><strong>Normalization:</strong> lowercase text; standardize whitespace.</li>
                <li><strong>Noise handling:</strong> keep punctuation handling minimal to avoid deleting signal.</li>
                <li><strong>Stopwords:</strong> treated as an ablation (we evaluate with/without).</li>
                <li><strong>Stemming/Lemmatization:</strong> optional ablation (can reduce interpretability).</li>
            </ul>
            <p class="footnote">
                Rationale: for topic classification on news text, overly aggressive preprocessing can remove useful named entities and phrases.
            </p>
            </div>

            <div class="card">
            <h3>Feature representations</h3>
            <ul>
                <li><strong>Bag-of-Words (BoW):</strong> count-based sparse vectors.</li>
                <li><strong>TF-IDF:</strong> reweights terms to emphasize discriminative vocabulary.</li>
                <li><strong>n-grams:</strong> unigram vs (1–2)-gram features to capture short phrases.</li>
            </ul>
            <p class="footnote">
                Typical parameters: <code>min_df</code> to remove rare noise terms, <code>max_df</code> to suppress very common terms, and a capped vocabulary size.
            </p>
            </div>
        </div>

        <div class="card">
            <h3>Models compared (≥ 3)</h3>
            <p>
            We compare three strong baselines for sparse text features:
            </p>
            <ul>
            <li><strong>Multinomial Naive Bayes (MNB):</strong> fast generative baseline; performs well on word-count features.</li>
            <li><strong>Logistic Regression (LR):</strong> linear discriminative model; interpretable weights and strong performance with TF-IDF.</li>
            <li><strong>Linear SVM (LinearSVC):</strong> often best classic baseline for high-dimensional sparse NLP features.</li>
            </ul>

            <div class="callout">
            <strong>Why these models?</strong> They are standard in text classification literature, scale well to sparse vectors, and allow clear comparisons.
            </div>
        </div>

        <div class="card">
            <h3>Comparison matrix (what we actually test)</h3>
            <ul>
            <li><strong>Features:</strong> BoW vs TF-IDF</li>
            <li><strong>n-grams:</strong> unigram vs (1–2)-grams</li>
            <li><strong>Models:</strong> MNB vs LR vs Linear SVM</li>
            </ul>
            <p class="footnote">
            All experiments are run with the same split protocol and the same metric set to ensure a fair comparison.
            </p>
        </div>
        </section>

      <section id="setup">
        <h2>5. Experimental Setup</h2>

        <p>
            We evaluate all model–feature combinations under a consistent protocol to ensure that improvements are due to
            modeling choices rather than differences in data splits or preprocessing.
        </p>

        <div class="grid2">
            <div class="card">
            <h3>Train / test strategy</h3>
            <ul>
                <li><strong>Split:</strong> stratified train/test (e.g., 80/20) with a fixed random seed.</li>
                <li><strong>Alternative:</strong> stratified k-fold cross-validation for more stable estimates.</li>
                <li><strong>Leakage control:</strong> apply vectorizer fitting only on the training portion.</li>
            </ul>
            </div>

            <div class="card">
            <h3>Metrics reported</h3>
            <ul>
                <li><strong>Accuracy:</strong> overall correctness.</li>
                <li><strong>Precision / Recall / F1:</strong> per-class behavior.</li>
                <li><strong>Macro-F1:</strong> treats both classes equally (recommended under imbalance).</li>
                <li><strong>Confusion matrix:</strong> interpretable error breakdown.</li>
            </ul>
            </div>
        </div>

        <div class="card">
            <h3>Hyperparameter policy (fair comparison)</h3>
            <ul>
            <li>Use a small, consistent tuning budget for each model (e.g., a limited grid).</li>
            <li>Keep the same split (or CV folds) across all experiments.</li>
            <li>Report the final chosen hyperparameters for reproducibility.</li>
            </ul>
            <p class="footnote">
            Example knobs: MNB <code>alpha</code>; LR/SVM regularization strength <code>C</code>; TF-IDF <code>ngram_range</code>, <code>min_df</code>, <code>max_df</code>.
            </p>
        </div>
        </section>

      <section id="conclusion">
        <h2>6. Conclusions</h2>

        <div class="card">
            <h3>What this study demonstrates</h3>
            <ul>
            <li>A simple ML pipeline can classify news text into <strong>Sports</strong> vs <strong>Politics</strong> reliably using sparse features.</li>
            <li><strong>TF-IDF + linear models</strong> (Logistic Regression / Linear SVM) are expected to be strong baselines.</li>
            <li><strong>n-grams</strong> help capture short phrases that are highly indicative of topic.</li>
            </ul>
        </div>

        <div class="card">
            <h3>Expected failure modes</h3>
            <ul>
            <li><strong>Mixed-topic articles:</strong> sports governance, funding, international events with political framing.</li>
            <li><strong>Entity-heavy text:</strong> country names and public figures can appear in both classes.</li>
            <li><strong>Short documents:</strong> limited vocabulary reduces confidence and increases ambiguity.</li>
            </ul>
        </div>

        <div class="callout">
            <strong>Next step:</strong> After training, we will add a results table (Accuracy + Macro-F1),
            a confusion matrix for the best model, and a brief error analysis with misclassified examples.
        </div>
        </section>

      

      

      

      <section id="repro">
        <h2>9. Reproducibility</h2>
        <p>
          Add: environment details, dataset link(s), and exact steps to reproduce figures and results.
        </p>
        <div class="callout">
          <strong>Planned:</strong> Provide a single command workflow (EDA → train → evaluate → export figures).
        </div>
      </section>

      <section id="refs">
        <h2>7. References</h2>

        <ul>
            <li>
            BBC Dataset distribution (Derek Greene), including copyright/use notes.
            </li>
            <li>
            BBC News Topic dataset card / CSV references used for loading and schema.
            </li>
            <li>
            Scikit-learn documentation: TF-IDF, Naive Bayes, Logistic Regression, Linear SVM, and evaluation metrics.
            </li>
        </ul>

        <div class="card">
            <h3>Citation-ready links (recommended)</h3>
            <ul>
            <li>Derek Greene — BBC datasets (source + usage notes)</li>
            <li>BBC News Topic dataset card (metadata + schema)</li>
            <li>scikit-learn user guide: text feature extraction (CountVectorizer / TfidfVectorizer)</li>
            <li>scikit-learn docs: MultinomialNB, LogisticRegression, LinearSVC</li>
            <li>scikit-learn docs: classification_report, confusion_matrix</li>
            </ul>
            <p class="footnote">
            Add direct URLs in your final version (and in the report) once you finalize your exact dataset download source and tools.
            </p>
        </div>
        </section>

      <footer class="footer">
        <p>© Omprakash Nain — Sports vs Politics Classifier</p>
      </footer>
    </article>
  </main>
</body>
</html>