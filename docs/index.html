<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Sports vs Politics — Text Classifier</title>
  <meta name="description" content="A classical NLP baseline study: BoW/TF-IDF + 3 ML models to classify documents as Sports or Politics." />
  <link rel="stylesheet" href="styles.css" />
</head>

<body>
  <header class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <span class="dot"></span>
        <span>Sports vs Politics Classifier</span>
      </div>
      <nav class="topnav">
        <a href="#dataset">Dataset</a>
        <a href="#eda">EDA</a>
        <a href="#methods">Methods</a>
        <a href="#results">Results</a>
        <a href="#repro">Repro</a>
        <a href="#refs">References</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <article class="post">
      <section class="hero">
        <h1>Sports vs Politics — Document Classification</h1>
        <p class="subtitle">
          A classical NLP baseline study using sparse text features (BoW / TF-IDF / n-grams) and 3+ ML models.
        </p>

        <div class="meta">
          <span>Author: <strong>[Your Name]</strong></span>
          <span>Course: <strong>[Course Name]</strong></span>
          <span>Date: <strong>[DD Mon YYYY]</strong></span>
        </div>

        <div class="callout">
          <strong>Goal:</strong> Build a classifier that reads a text document and labels it as either
          <strong>Sports</strong> or <strong>Politics</strong>, then compare at least three ML techniques quantitatively.
        </div>
      </section>

      <aside class="toc">
        <h2>Table of contents</h2>
        <ol>
          <li><a href="#problem">Problem</a></li>
          <li><a href="#dataset">Dataset</a></li>
          <li><a href="#eda">Exploratory Data Analysis</a></li>
          <li><a href="#methods">Feature Representations + Models</a></li>
          <li><a href="#setup">Experimental Setup</a></li>
          <li><a href="#results">Results</a></li>
          <li><a href="#errors">Error Analysis</a></li>
          <li><a href="#limits">Limitations</a></li>
          <li><a href="#repro">Reproducibility</a></li>
          <li><a href="#refs">References</a></li>
        </ol>
      </aside>

      <section id="problem">
        <h2>1. Problem</h2>
        <p>
          We solve a binary text classification task:
          given a document <em>x</em>, predict <em>y ∈ {sports, politics}</em>.
        </p>
        <p>
          This project intentionally uses simple, explainable NLP baselines (BoW/TF-IDF + linear models),
          because they are strong for topic classification and easy to analyze.
        </p>
      </section>

      <section id="dataset">
        <h2>2. Dataset</h2>

        <p>
            We use the BBC News Topic dataset (2004–2005), a popular benchmark for topic classification.
            It contains <strong>2,225</strong> articles labeled into five categories:
            business, entertainment, politics, sport, and tech.
        </p>

        <div class="callout">
            <strong>Binary task construction:</strong> We filter the dataset to only
            <code>sport</code> and <code>politics</code> to match the assignment statement exactly.
            The resulting dataset contains <strong>928 documents</strong>:
            <strong>511 sport</strong> and <strong>417 politics</strong>.
        </div>

        <div class="grid2">
            <div class="card">
            <h3>Data source</h3>
            <ul>
                <li>
                Practical download: Kaggle CSV commonly distributed as <code>bbc-text.csv</code>
                with columns like <code>category</code> and <code>text</code>.
                </li>
                <li>
                Original dataset distribution is maintained by Derek Greene as “BBC Datasets”
                for research/benchmarking.
                </li>
                <li>
                Copyright note: the original article content is owned by the BBC, so we do not
                re-upload the raw text in this repository.
                </li>
            </ul>
            </div>

            <div class="card">
            <h3>Schema</h3>
            <table>
                <thead>
                <tr>
                    <th>Column</th>
                    <th>Meaning</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td><code>category</code></td>
                    <td>One of {business, entertainment, politics, sport, tech}</td>
                </tr>
                <tr>
                    <td><code>text</code></td>
                    <td>Full article text</td>
                </tr>
                </tbody>
            </table>
            <p class="footnote">
                We only keep rows where <code>category ∈ {sport, politics}</code>.
            </p>
            </div>
        </div>

        <div class="card">
            <h3>Collection and curation procedure</h3>
            <ol>
            <li><strong>Download</strong> the CSV dataset locally (do not commit it to GitHub).</li>
            <li><strong>Validate schema</strong>: ensure required columns exist and remove null/empty texts.</li>
            <li><strong>Deduplicate</strong> (recommended): drop exact duplicates of <code>text</code>.</li>
            <li><strong>Filter classes</strong>: keep only <code>sport</code> and <code>politics</code>.</li>
            <li>
                <strong>Split</strong>: use a stratified split to preserve class ratio
                (e.g., 80/20 with a fixed random seed) or stratified k-fold cross-validation.
            </li>
            </ol>
        </div>

        <figure class="figure">
            <img src="assets/figures/class_balance.png" alt="Class balance bar chart" />
            <figcaption>
            <strong>Figure:</strong> Class distribution after filtering (sport vs politics).
            </figcaption>
        </figure>

        <figure class="figure">
            <img src="assets/figures/word_count_dist.png" alt="Word count distribution plot" />
            <figcaption>
            <strong>Figure:</strong> Document length distribution (word counts) by class.
            </figcaption>
        </figure>
        </section>

      <section id="eda">
        <h2>3. Exploratory Data Analysis (EDA)</h2>

        <p>
            Before training models, we examine (i) class balance, (ii) document length distribution, and
            (iii) vocabulary signals. This sanity-checks the dataset and motivates our feature choices
            (BoW vs TF-IDF, unigrams vs n-grams).
        </p>

        <div class="grid2">
            <div class="card">
            <h3>What we check in EDA</h3>
            <ul>
                <li><strong>Balance:</strong> do we have enough samples per class?</li>
                <li><strong>Length:</strong> do classes differ in typical document size?</li>
                <li><strong>Vocabulary:</strong> which words appear frequently within each class?</li>
                <li><strong>Ambiguity:</strong> are there “mixed-topic” documents?</li>
            </ul>
            </div>

            <div class="card">
            <h3>Design decisions driven by EDA</h3>
            <ul>
                <li>Use <strong>stratified splits</strong> (preserve class ratio in train/test).</li>
                <li>Prefer <strong>TF-IDF</strong> as a strong baseline over raw counts.</li>
                <li>Evaluate both <strong>unigrams</strong> and <strong>n-grams</strong> (phrases can be highly informative).</li>
                <li>Report <strong>Macro-F1</strong> along with accuracy (more robust if imbalance exists).</li>
            </ul>
            </div>
        </div>

        <figure class="figure">
            <img src="assets/figures/class_balance.png" alt="Class balance bar chart" />
            <figcaption>
            <strong>EDA 1:</strong> Class balance (sport vs politics). We preserve this ratio using a stratified split.
            </figcaption>
        </figure>

        <figure class="figure">
            <img src="assets/figures/word_count_dist.png" alt="Word count distribution plot" />
            <figcaption>
            <strong>EDA 2:</strong> Document length distribution (word counts) by class.
            Significant overlap suggests that vocabulary (not length alone) drives classification.
            </figcaption>
        </figure>

        <div class="grid2">
            <div class="card">
            <h3>What length tells us</h3>
            <ul>
                <li>If one class is consistently shorter, models might partially exploit length as a shortcut.</li>
                <li>Large overlap usually implies the discriminative signal is mostly lexical/phrase-level.</li>
                <li>Very short documents are typically harder due to low information content.</li>
            </ul>
            </div>

            <div class="card">
            <h3>Vocabulary signals (quick sanity check)</h3>
            <ul>
                <li>Sports should surface match/event terminology (e.g., teams, scores, tournaments).</li>
                <li>Politics should surface institutional terminology (e.g., parties, elections, governance).</li>
                <li>Overlap (e.g., “campaign”, “win”, country names) can create confusion cases.</li>
            </ul>
            </div>
        </div>

        <h3>EDA 3: Word clouds (class vocabulary snapshot)</h3>
        <p>
            Word clouds provide a fast qualitative view of high-frequency vocabulary within each class.
            While not a quantitative feature-importance method, they are useful for sanity-checking:
            the dominant words should align with the intended labels.
        </p>

        <div class="grid2">
            <figure class="figure">
            <img src="assets/figures/wordcloud_sport.png" alt="Word cloud for Sport class" />
            <figcaption><strong>EDA 3a:</strong> Word cloud for <code>sport</code>.</figcaption>
            </figure>

            <figure class="figure">
            <img src="assets/figures/wordcloud_politics.png" alt="Word cloud for Politics class" />
            <figcaption><strong>EDA 3b:</strong> Word cloud for <code>politics</code>.</figcaption>
            </figure>
        </div>

        <div class="callout">
            <strong>Important note:</strong> Word clouds are qualitative.
            Our final conclusions will rely on quantitative evaluation of models trained with BoW/TF-IDF and n-grams.
        </div>

        <div class="card">
            <h3>EDA summary</h3>
            <ul>
            <li>We preserve class ratio with a stratified split and report Accuracy + Macro-F1.</li>
            <li>Document length overlap suggests vocabulary/phrases are the main discriminative signal.</li>
            <li>Next, we compare BoW vs TF-IDF and at least three ML models under the same evaluation protocol.</li>
            </ul>
        </div>
        </section>

      <section id="methods">
        <h2>4. Feature Representations + Models</h2>

        <h3>Feature representations</h3>
        <ul>
          <li><strong>Bag-of-Words (BoW):</strong> raw token counts.</li>
          <li><strong>TF-IDF:</strong> downweights frequent words, upweights discriminative terms.</li>
          <li><strong>n-grams:</strong> capture short phrases (e.g., “prime minister”, “world cup”).</li>
        </ul>

        <h3>Models compared (at least 3)</h3>
        <div class="grid3">
          <div class="card">
            <h4>Multinomial Naive Bayes</h4>
            <p>Strong baseline for sparse counts; fast and surprisingly competitive.</p>
          </div>
          <div class="card">
            <h4>Logistic Regression</h4>
            <p>Linear classifier with calibrated probabilities; very interpretable weights.</p>
          </div>
          <div class="card">
            <h4>Linear SVM</h4>
            <p>Often best-performing classic baseline for text classification.</p>
          </div>
        </div>

        <div class="callout">
          <strong>Note:</strong> We keep preprocessing minimal to avoid deleting signal (e.g., named entities).
          Stopword removal and stemming are treated as “ablation experiments” rather than defaults.
        </div>
      </section>

      <section id="setup">
        <h2>5. Experimental Setup</h2>
        <ul>
          <li><strong>Split strategy:</strong> stratified train/test (e.g., 80/20) OR 5-fold stratified CV.</li>
          <li><strong>Metrics:</strong> Accuracy + Precision/Recall/F1 (macro) + confusion matrix.</li>
          <li><strong>Fair comparison:</strong> same split, same preprocessing, tune hyperparameters consistently.</li>
        </ul>
      </section>

      <section id="results">
        <h2>6. Results</h2>

        <p>
          This section will contain your final numbers and visuals after training.
        </p>

        <figure class="figure">
          <img src="assets/figures/model_comparison.png" alt="Model comparison chart" />
          <figcaption><strong>Figure:</strong> Model comparison (Accuracy / F1). (Generate this plot.)</figcaption>
        </figure>

        <figure class="figure">
          <img src="assets/figures/confusion_matrix.png" alt="Confusion matrix" />
          <figcaption><strong>Figure:</strong> Confusion matrix for the best model. (Generate this plot.)</figcaption>
        </figure>

        <div class="card">
          <h3>Result table (fill after experiments)</h3>
          <table>
            <thead>
              <tr>
                <th>Features</th>
                <th>Model</th>
                <th>Accuracy</th>
                <th>F1 (macro)</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>BoW</td><td>Naive Bayes</td><td>—</td><td>—</td></tr>
              <tr><td>TF-IDF</td><td>Logistic Regression</td><td>—</td><td>—</td></tr>
              <tr><td>TF-IDF (1–2 grams)</td><td>Linear SVM</td><td>—</td><td>—</td></tr>
            </tbody>
          </table>
        </div>
      </section>

      <section id="errors">
        <h2>7. Error Analysis</h2>
        <ul>
          <li>Show 5–10 misclassified examples and explain why they’re hard.</li>
          <li>Look for overlap topics (e.g., “government funding of sports”, “international events”).</li>
          <li>Check if headlines-only vs full-article changes difficulty (if you experiment).</li>
        </ul>
      </section>

      <section id="limits">
        <h2>8. Limitations</h2>
        <ul>
          <li><strong>Dataset age:</strong> articles are from 2004–2005; language and entities shift over time.</li>
          <li><strong>Topic ambiguity:</strong> sports governance/policy can resemble politics text.</li>
          <li><strong>Domain shift:</strong> models trained on news may not generalize to tweets/comments.</li>
        </ul>
      </section>

      <section id="repro">
        <h2>9. Reproducibility</h2>
        <p>
          Add: environment details, dataset link(s), and exact steps to reproduce figures and results.
        </p>
        <div class="callout">
          <strong>Planned:</strong> Provide a single command workflow (EDA → train → evaluate → export figures).
        </div>
      </section>

      <section id="refs">
        <h2>10. References</h2>
        <ul>
          <li>BBC dataset description / benchmark repository</li>
          <li>Any Kaggle notebook(s) you used for EDA inspiration</li>
          <li>Scikit-learn docs for models/metrics</li>
        </ul>

        <p class="footnote">
          Dataset note: all rights in the original BBC article content belong to the BBC (as stated by dataset distributors).
        </p>
      </section>

      <footer class="footer">
        <p>© [Your Name] — Sports vs Politics Classifier</p>
      </footer>
    </article>
  </main>
</body>
</html>